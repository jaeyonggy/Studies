<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Bayesian Statistics</title>

<script src="Bayesian_files/header-attrs-2.11/header-attrs.js"></script>
<script src="Bayesian_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Bayesian_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Bayesian_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Bayesian_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Bayesian_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="Bayesian_files/navigation-1.1/tabsets.js"></script>
<link href="Bayesian_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Bayesian_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Bayesian Statistics</h1>

</div>


<p><br />
<br />
<br />
</p>
<div id="posterior-interval-by-numerical-method" class="section level2">
<h2>Posterior Interval by ‘Numerical Method’</h2>
<p><br />
</p>
<pre class="r"><code>### Placenta Previa example
y=seq(0,1,by=0.001)

plot(y,dbeta(y,438,544),type=&quot;l&quot;,xlim=c(0.3,0.6))  #Posterior distribution</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>post.y=rbeta(1000,438,544)  #Taking 1000 samples from the Post. dstn

dev.new()
hist(post.y,freq=F)  #Plotting the sample

mean(post.y)  #The posterior mean</code></pre>
<pre><code>## [1] 0.4469573</code></pre>
<pre class="r"><code>quantile(post.y,probs=c(0.025,0.975))  # 95% Posterior Interval using Numerical Method</code></pre>
<pre><code>##      2.5%     97.5% 
## 0.4186769 0.4775547</code></pre>
<p><br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
<div id="normal-model-with-discrete-prior-distribution-estimating-the-mean" class="section level2">
<h2>Normal Model With Discrete Prior Distribution (Estimating the mean)</h2>
<p><br />
</p>
<pre class="r"><code>options(scipen=999)  #To disable printing your results in scientific notation.

mu=c(20,30,40,50,60,70)  #Parameter of interest
prior=c(0.1,0.15,0.25,0.25,0.15,0.1)  #Discrete Prior Distribution
y=c(38.6,42.4,57.5,40.5,51.7,67.1,33.4,60.9,64.1,40.1,40.7,6.4)  #Observations (n=10)

ybar=mean(y)
sigma=10  #Known population variance
n=length(y)
like=exp((-n/(2*sigma^2))*(mu-ybar)^2)  #Likelihood function (Refer to the notes)

unn.post=prior*like  #Unnormalized Discrete Posterior dstn 
unn.post</code></pre>
<pre><code>## [1] 0.00000000000000000220148 0.00000012289485905573538
## [3] 0.04683563061571816704687 0.06580160638607958356605
## [5] 0.00000034081137965149324 0.00000000000000001205079</code></pre>
<pre class="r"><code>nor.post=unn.post/sum(unn.post)  #Normalized Discrete Posterior dstn (We have to Normalize it to make the Probability Distribution integrate to 1)
nor.post  #We can see that the Discrete Post. dstn is [20: 0, 30: 0, 40: 0.42, 50: 0.58, 60: 0, 70: 0]</code></pre>
<pre><code>## [1] 0.00000000000000001954479 0.00000109106327884201111
## [3] 0.41580776526252849478738 0.58418811794322056396567
## [5] 0.00000302573097203836150 0.00000000000000010698715</code></pre>
<pre class="r"><code>dev.new()
plot(mu,nor.post)  #We can see that the most likely values are 40 and 50

s.post=sample(mu,1000,prob=nor.post,replace=T)  #Take a sample from the Post. dstn
table(s.post)  #The number of observations from values of 40 and 50</code></pre>
<pre><code>## s.post
##  40  50 
## 392 608</code></pre>
<pre class="r"><code>mean(s.post)</code></pre>
<pre><code>## [1] 46.08</code></pre>
<pre class="r"><code>sd(s.post)</code></pre>
<pre><code>## [1] 4.88441</code></pre>
<p><br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
<div id="frequentist-vs-bayesian-estimators-by-mse" class="section level2">
<h2>Frequentist vs Bayesian Estimators (by MSE)</h2>
<p><br />
</p>
<pre class="r"><code>#Sample Size is SMALL
n=10
theta=seq(0,1,by=0.01)  #Generate values of theta from (0,1) to see the MSE at each value of theta
#Refer to the notes for the equation of the MSEs
mse.f=theta*(1-theta)/n  #MSE of the Frequentist estimator at each value of theta
mse.b=((4-n)*theta^2-(4-n)*theta+1)/(n+2)^2  #MSE of the Bayesian estimator(=Posterior Mean) at each value of theta

dev.new()
plot(theta,mse.f,type=&quot;l&quot;)
lines(theta,mse.b,col=&quot;red&quot;)
legend(x=&quot;topright&quot;, legend=c(&quot;Freq.&quot;,&quot;Bayes.&quot;), lty=c(1,1), col=c(&quot;black&quot;,&quot;red&quot;))  </code></pre>
<div id="we-can-see-that-the-bayes-mses-are-smaller-in-general-when-the-sample-size-is-small" class="section level4">
<h4>We can see that the Bayes’ MSEs are smaller in general when the sample size is small</h4>
<pre class="r"><code>#Sample Size is LARGE
n=1000
theta=seq(0,1,by=0.01)  #Generate values of theta from (0,1) to see the MSE at each value of theta
#Refer to the notes for the equation of the MSEs
mse.f=theta*(1-theta)/n  #MSE of the Frequentist estimator at each value of theta
mse.b=((4-n)*theta^2-(4-n)*theta+1)/(n+2)^2  #MSE of the Bayesian estimator(=Posterior Mean) at each value of theta

dev.new()
plot(theta,mse.f,type=&quot;l&quot;)
lines(theta,mse.b,col=&quot;red&quot;)
legend(x=&quot;topright&quot;, legend=c(&quot;Freq.&quot;,&quot;Bayes.&quot;), lty=c(1,1), col=c(&quot;black&quot;,&quot;red&quot;))  </code></pre>
</div>
<div id="we-can-see-that-the-mses-of-the-two-estimators-are-almost-identical-when-the-sample-size-is-large" class="section level4">
<h4>We can see that the MSEs of the two estimators are almost identical when the sample size is large</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="highest-posterior-density-hpd-credible-set" class="section level2">
<h2>Highest Posterior Density (HPD) Credible Set</h2>
<p><br />
</p>
<p><img src="images/IMG_0086.jpeg" /></p>
<p><br />
</p>
<div id="credible-interval-is" class="section level4">
<h4>95% Credible Interval is</h4>
<pre class="r"><code>n = 10
y = 2

ci = qbeta(c(0.025,0.975), y+1, n-y+1)
ci</code></pre>
<pre><code>## [1] 0.06021773 0.51775585</code></pre>
<p><br />
</p>
</div>
<div id="hpd-interval-is" class="section level4">
<h4>95% HPD interval is</h4>
<pre class="r"><code># install.packages(&quot;TeachingDemos&quot;)  # hpd function is in there
library(TeachingDemos)

hpd=hpd(qbeta,shape1=y+1,shape2=n-y+1)
hpd</code></pre>
<pre><code>## [1] 0.04055517 0.48372366</code></pre>
<p><br />
</p>
</div>
<div id="visualization" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>theta = seq(0, 1, by=0.01)

plot(theta,dbeta(theta,y+1,n-y+1),type=&quot;l&quot;,xlab=expression(theta),ylab=&quot;Posterior Density Function&quot;, cex.lab=1)

segments(ci[1],0,ci[1],dbeta(ci[1],y+1,n-y+1),col=&quot;red&quot;,lwd=2)
segments(ci[2],0,ci[2],dbeta(ci[2],y+1,n-y+1),col=&quot;red&quot;,lwd=2)
segments(ci[1],dbeta(ci[1],y+1,n-y+1),ci[2],dbeta(ci[2],y+1,n-y+1),col=&quot;red&quot;,lwd=2)

# HPD
segments(hpd[1],0,hpd[1],dbeta(hpd[1],y+1,n-y+1),col=&quot;blue&quot;,lwd=2,lty=2)
segments(hpd[2],0,hpd[2],dbeta(hpd[2],y+1,n-y+1),col=&quot;blue&quot;,lwd=2,lty=2)
segments(hpd[1],dbeta(hpd[1],y+1,n-y+1),hpd[2],dbeta(hpd[2],y+1,n-y+1),col=&quot;blue&quot;,lwd=2,lty=2)
legend(x=0.6,y=3,legend=c(&quot;95% CI=(0.06,0.52)&quot;, &quot;95% HPD CI=(0.04,0.48)&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),lty=c(1,2),cex=0.8)</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="calculating-posterior-predictive-p-values-bayesian-p-values" class="section level2">
<h2>Calculating Posterior Predictive P-values (Bayesian P-Values)</h2>
<p><br />
</p>
<p><img src="images/IMG_0083.jpeg" /></p>
<p><img src="images/IMG_0084.jpeg" /></p>
<p><img src="images/IMG_0085.jpeg" /></p>
<p><br />
</p>
<pre class="r"><code>nsim=100

y=c(1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0)
T.y=sum(diff(y)!=0)
theta=rep(NA,nsim)
T.y.rep=rep(NA,nsim)

for (i in 1:nsim){
  theta[i]&lt;-rbeta(1,8,14)
  y.rep&lt;-rbinom(20,1,theta[i])
  T.y.rep[i]&lt;-sum(diff(y.rep)!=0)
}

sum(T.y.rep&lt;=T.y)/nsim  # Bayesian P-value</code></pre>
<pre><code>## [1] 0.02</code></pre>
<div id="this-is-the-bayesian-p-value-after-100-simulations" class="section level4">
<h4>This is the Bayesian p-value (after 100 simulations)</h4>
<p><br />
</p>
<pre class="r"><code>hist(T.y.rep)  # Posterior predictive distribution of the replicated test statistic
abline(v=3,col=&quot;red&quot;)  # The value left to the red line is the p-value</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="posterior-dstn-for-multinomial-model-for-categorical-data" class="section level2">
<h2>Posterior dstn for Multinomial model for Categorical data</h2>
<p><br />
</p>
<p><img src="images/ss.png" /></p>
<p><br />
</p>
<div id="lets-calculate-the-post.-dstn-for-theta1---theta2.-lets-call-this-diff" class="section level3">
<h3>Let’s calculate the post. dstn for theta1 - theta2. (Let’s call this diff)</h3>
<p><br />
</p>
</div>
<div id="were-gonna-take-samples-from-the-post.-dstn-of-theta1theta2theta3-and-calculate-it." class="section level3">
<h3>We’re gonna take samples from the post. dstn of (theta1,theta2,theta3) and calculate it.</h3>
<p><br />
</p>
<pre class="r"><code>library(MCMCpack)</code></pre>
<pre><code>## Loading required package: coda</code></pre>
<pre><code>## Loading required package: MASS</code></pre>
<pre><code>## ##
## ## Markov Chain Monte Carlo Package (MCMCpack)</code></pre>
<pre><code>## ## Copyright (C) 2003-2021 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park</code></pre>
<pre><code>## ##
## ## Support provided by the U.S. National Science Foundation</code></pre>
<pre><code>## ## (Grants SES-0350646 and SES-0350613)
## ##</code></pre>
<pre class="r"><code>n=1447

y1=727
y2=583
y3=137

alpha=c(y1+1,y2+1,y3+1)  # the parameters of post. dstn of (theta1,theta2,theta3)

nsim=100  # number of simulations    

theta=rdirichlet(nsim,alpha)  # Taking samples from the post. dstn of (theta1,theta2,theta3)
diff=theta[,1]-theta[,2]
diff  # theta1 - theta2</code></pre>
<pre><code>##   [1] 0.11299836 0.08039456 0.08795084 0.13310991 0.15000374 0.06517455
##   [7] 0.14086325 0.13134798 0.07847545 0.11753060 0.06802134 0.12642999
##  [13] 0.09394026 0.10725070 0.09626422 0.11249895 0.09480357 0.12700254
##  [19] 0.10890444 0.10198936 0.10003833 0.14903969 0.09437602 0.11005913
##  [25] 0.10907928 0.08186004 0.09876473 0.07427015 0.05694277 0.07819730
##  [31] 0.06994722 0.10215176 0.09442923 0.12346479 0.10518044 0.08407032
##  [37] 0.10793076 0.09383968 0.08118693 0.09976934 0.05872758 0.11690948
##  [43] 0.05228263 0.08827325 0.09277832 0.11201357 0.09466478 0.12430119
##  [49] 0.05365929 0.08129438 0.11312447 0.05342744 0.09092329 0.10237039
##  [55] 0.06663086 0.07421578 0.08108636 0.11845503 0.11742938 0.03870028
##  [61] 0.09557757 0.10231140 0.15675853 0.10298068 0.12300478 0.08640578
##  [67] 0.04233871 0.09445658 0.10760836 0.09382812 0.08326581 0.11528917
##  [73] 0.13630141 0.12101308 0.11148380 0.10213602 0.14409597 0.12933027
##  [79] 0.12601939 0.09346302 0.13261181 0.10804757 0.10355153 0.08414038
##  [85] 0.08197786 0.09070102 0.08535921 0.14370082 0.05547138 0.15440351
##  [91] 0.10950084 0.09959808 0.07316866 0.11444356 0.09287810 0.10437962
##  [97] 0.16663463 0.11860176 0.06856669 0.08059077</code></pre>
<p><br />
</p>
</div>
<div id="histogram-plot-of-dstn-for-diff" class="section level3">
<h3>Histogram plot of dstn for diff</h3>
<p><br />
</p>
<pre class="r"><code>hist(diff)</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><br />
</p>
</div>
<div id="point-estimate" class="section level3">
<h3>Point Estimate</h3>
<p><br />
</p>
<pre class="r"><code>mean(diff)</code></pre>
<pre><code>## [1] 0.1001682</code></pre>
<p><br />
</p>
</div>
<div id="interval-estimate-alpha-0.05" class="section level3">
<h3>Interval Estimate (alpha = 0.05)</h3>
<p><br />
</p>
<pre class="r"><code>quantile(diff,probs=c(0.025,0.975))  # Equal tailed interval for alpha = 0.05</code></pre>
<pre><code>##       2.5%      97.5% 
## 0.05282641 0.15231362</code></pre>
<p><br />
</p>
</div>
<div id="the-count-of-theta1-theta2" class="section level3">
<h3>The count of theta1 &lt; theta2</h3>
<p><br />
</p>
<pre class="r"><code>sum(diff&lt;0)</code></pre>
<pre><code>## [1] 0</code></pre>
<p><br />
</p>
</div>
<div id="we-can-see-that-theta1-is-larger-than-theta2." class="section level3">
<h3>We can see that theta1 is larger than theta2.</h3>
<p><br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="monte-carlo-integration" class="section level2">
<h2>Monte Carlo Integration</h2>
<p><br />
</p>
<div id="case-i" class="section level3">
<h3>Case I</h3>
<p><br />
</p>
<p><img src="images/IMG_0153.jpeg" /></p>
<p><img src="images/IMG_0152.jpeg" /></p>
<p><br />
</p>
</div>
<div id="true-value" class="section level3">
<h3>True Value</h3>
<p><br />
</p>
<pre class="r"><code>2/3</code></pre>
<pre><code>## [1] 0.6666667</code></pre>
<p><br />
</p>
</div>
<div id="direct-computation" class="section level3">
<h3>Direct Computation</h3>
<pre class="r"><code>N&lt;-100000
theta&lt;-runif(N,-1,1)  # Drawing N number of values
h.theta&lt;-2*theta^2   # The h() function

mu.h.theta&lt;-mean(h.theta)   # MC estimate
mu.h.theta</code></pre>
<pre><code>## [1] 0.6657692</code></pre>
<p><br />
</p>
<div id="pretty-close-to-the-true-value." class="section level4">
<h4>Pretty close to the true value.</h4>
<p><br />
</p>
</div>
</div>
<div id="iteration-method" class="section level3">
<h3>Iteration Method</h3>
<pre class="r"><code>N=10000
theta=rep(0,N)  # Initializing the places to save the values I guess
h.theta=rep(0,N)
mu.h.theta=rep(0,N)

for (i in 1:N){
  theta[i]=runif(1,-1,1)  # Drawing 1 value at a time
  h.theta[i]=2*theta[i]^2
  mu.h.theta[i]=mean(h.theta[1:i])
}

mu.h.theta[N]  # MC estimate</code></pre>
<pre><code>## [1] 0.6598238</code></pre>
<pre class="r"><code>dev.new()
plot(1:N,mu.h.theta,type=&quot;l&quot;)</code></pre>
<div id="the-htheta-eventually-converges-to-the-true-value." class="section level4">
<h4>The h(theta) eventually converges to the true value.</h4>
<p><br />
<br />
<br />
</p>
</div>
</div>
<div id="case-ii" class="section level3">
<h3>Case II</h3>
<p><br />
</p>
<p><img src="images/IMG_0154.jpeg" /></p>
<p><img src="images/IMG_0155.jpeg" /></p>
<p><br />
</p>
</div>
<div id="true-value-1" class="section level3">
<h3>True Value</h3>
<pre class="r"><code>1-exp(-1)</code></pre>
<pre><code>## [1] 0.6321206</code></pre>
<p><br />
</p>
</div>
<div id="direct-computation-1" class="section level3">
<h3>Direct Computation</h3>
<pre class="r"><code>N=10000
x=runif(N)  # Drawing N number of values

# g(x) = exp(-x)
theta.hat=mean(exp(-x))  # MC estimate
theta.hat</code></pre>
<pre><code>## [1] 0.6337929</code></pre>
<div id="pretty-close-to-the-true-value.-1" class="section level4">
<h4>Pretty close to the true value.</h4>
<p><br />
</p>
</div>
</div>
<div id="iteration-method-1" class="section level3">
<h3>Iteration method</h3>
<pre class="r"><code>N=10000
x=rep(0,N)  # Initializing the places to save the values I guess
h.x=rep(0,N)
mu.h.x=rep(0,N)

for (i in 1:N){
  x[i]=runif(1)
  h.x[i]=exp(-x[i])
  mu.h.x[i]=mean(h.x[1:i])
}

mu.h.x[N]  # MC estimate</code></pre>
<pre><code>## [1] 0.6322952</code></pre>
<pre class="r"><code>dev.new()
plot(1:N,mu.h.x,type=&quot;l&quot;)</code></pre>
<div id="it-eventually-converges-to-the-true-value." class="section level4">
<h4>It eventually converges to the true value.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="rejection-sampling" class="section level2">
<h2>Rejection Sampling</h2>
<p><br />
</p>
<p><img src="images/IMG_0157.jpeg" /></p>
<p><br />
</p>
<div id="target-density-p" class="section level3">
<h3>Target Density, p</h3>
<pre class="r"><code>p &lt;- function(x) {
  if (x &gt;= 0 &amp;&amp; x &lt; 0.25)
     8 * x
  else if (x &gt;= 0.25 &amp;&amp; x &lt;= 1)
     8/3 - 8 * x/3
   else 0
  }</code></pre>
<p><br />
</p>
</div>
<div id="proposal-density-g-and-constant-m" class="section level3">
<h3>Proposal Density, g and Constant M</h3>
<pre class="r"><code>g &lt;- function(x) {
   if (x &gt;= 0 &amp;&amp; x &lt;= 1)
     1
   else 0
  }

M&lt;-3  # Try out different values for M (M is better to be smaller)</code></pre>
<p><br />
</p>
</div>
<div id="acceptreject-algorithm" class="section level3">
<h3>Accept/Reject algorithm</h3>
<pre class="r"><code>m &lt;- 1000  # Total number of accepted values needed
n.draws &lt;- 0  # For counting the number of accepted values
draws &lt;- c()  # Place to store accepted values

while (n.draws &lt; m) {
   x.c &lt;- runif(1, 0, 1)  # Drawing a candidate from the proposal density
   accept.prob &lt;- p(x.c)/(M * g(x.c))  # The prob. of accepting for each candidate
   u &lt;- runif(1, 0, 1)  # The auxiliary variable used for accept/reject algo.
   
   if (accept.prob &gt;= u) {
     draws &lt;- c(draws, x.c)  # Storing the accepted candidate
     n.draws &lt;- n.draws + 1  # Counting the accepted candidate
     }
   }</code></pre>
<p><br />
</p>
</div>
<div id="the-accepted-values" class="section level3">
<h3>The accepted values</h3>
<pre class="r"><code>head(draws)</code></pre>
<pre><code>## [1] 0.7028893 0.2497072 0.6639698 0.4258803 0.1941240 0.3663159</code></pre>
<p><br />
</p>
</div>
<div id="visualization-1" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code># Initializing variables for the plots
x &lt;- seq(0, 1, by = 0.01)
y=rep(0,length(x))
for (i in 1:length(x)){
  y[i]=p(x[i])
}

dev.new()
par(mfrow=c(1,3))
plot(x,y,type=&quot;l&quot;,ylim=c(0,4),col=&quot;black&quot;, main=&quot;Plot of p() and Mg()&quot;)
abline(h=M,col=&quot;red&quot;)
legend(&quot;topright&quot;, legend=c(&quot;p()&quot;, &quot;Mg()&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=1, cex=0.8)

plot(x,y,type=&quot;l&quot;,ylim=c(0,4),col=&quot;black&quot;, main=&quot;Plot of p() and the density\n of the accepted values&quot;)
lines(density(draws),col=&quot;red&quot;)
legend(&quot;topright&quot;, legend=c(&quot;p()&quot;, &quot;acc&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=1, cex=0.8)

hist(draws,freq=F,ylim=c(0,4), xlab=&quot;x&quot;, ylab=&quot;y&quot;, main=&quot;Histogram of accepted values&quot;)
lines(x,y,type=&quot;l&quot;,col=&quot;black&quot;)</code></pre>
<p><br />
<br />
<br />
</p>
<p><img src="images/IMG_0158.jpeg" /></p>
<p><br />
</p>
</div>
<div id="target-density-p-1" class="section level3">
<h3>Target Density, p</h3>
<pre class="r"><code>x.grid=seq(-5,5,by=0.01)
p=dnorm(x.grid,0,1)</code></pre>
<p><br />
</p>
</div>
<div id="proposal-density-g-and-constant-m." class="section level3">
<h3>Proposal Density, g and Constant M.</h3>
<pre class="r"><code>g=dnorm(x.grid,0,2)
M=2</code></pre>
<p><br />
</p>
</div>
<div id="acceptreject-algorithm-1" class="section level3">
<h3>Accept/Reject algorithm</h3>
<pre class="r"><code>N=10000  # Total number of iterations
         # This time we will be checking the general acceptance rate
x=rep(0,N)  # Places to store the accepted values
            # The unaccepted ones will be 0 so we can count how many nonzeros there
            # out of N to calculate the general acceptance rate

for (i in 1:(N)){
  x_star=rnorm(1,0,2)  # Drawing a candidate from the proposal density
  accept.prob=dnorm(x_star,0,1)/(M*dnorm(x_star,0,2))  # Prob. of accepting
  U=runif(1,0,1)  # The auxiliary variable
  if (accept.prob&gt;=U){x[i]=x_star}
}</code></pre>
<p><br />
</p>
</div>
<div id="the-general-acceptance-rate" class="section level3">
<h3>The general acceptance rate</h3>
<div id="i.e.-how-many-values-were-accepted-out-of-n-candidates" class="section level4">
<h4>i.e. how many values were accepted out of N candidates</h4>
<pre class="r"><code>accept.rate=sum(x!=0)/N
accept.rate</code></pre>
<pre><code>## [1] 0.5032</code></pre>
</div>
<div id="from-this-we-can-deduce-that-the-number-of-accepted-values-is-about-half-of-n" class="section level4">
<h4>From this we can deduce that the number of accepted values is about half of N</h4>
<p><br />
</p>
</div>
</div>
<div id="visualization-2" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(1,3))
plot(x.grid,p,type=&quot;l&quot;,ylim=c(0,0.6), xlab=&quot;x&quot;, ylab=&quot;y&quot;, main=&quot;Plot of p() and Mg()&quot;)
lines(x.grid,M*g,type=&quot;l&quot;,col=&quot;red&quot;)
legend(&quot;topright&quot;, legend=c(&quot;p()&quot;, &quot;Mg()&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=1, cex=0.8)


nonzero = x[x!=0]  # We don&#39;t need 0s to plot the density

plot(x.grid,p,type=&quot;l&quot;, main=&quot;Plot of p() and the density\n of the accepted values&quot;)
lines(density(nonzero),col=&quot;red&quot;)
legend(&quot;topright&quot;, legend=c(&quot;p()&quot;, &quot;acc&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=1, cex=0.8)

hist(nonzero,freq=F,ylim=c(0,0.5), ylab=&quot;y&quot;, main=&quot;Histogram of accepted values&quot;)
lines(x.grid, dnorm(x.grid,0,1))</code></pre>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="importance-sampling" class="section level2">
<h2>Importance Sampling</h2>
<p><img src="images/IMG_0160.jpeg" /></p>
<p><br />
</p>
<div id="true-value-2" class="section level3">
<h3>True Value</h3>
<pre class="r"><code>1-pnorm(3)</code></pre>
<pre><code>## [1] 0.001349898</code></pre>
<p><br />
</p>
</div>
<div id="importance-sampling-estimate" class="section level3">
<h3>Importance Sampling Estimate</h3>
<pre class="r"><code>N=100000  # Number of samples to extract from the proposal dstn
x=rnorm(N,4,1)  # Samples from the proposal dstn
f.x=dnorm(x,0,1)  # To calculate the weight ftn
g.x=dnorm(x,4,1)  # To calculate the weight ftn
h.x=as.numeric(x&gt;=3)  # Target
w.x=f.x/g.x  # Weight ftn

h.x_IS=mean(h.x*w.x)  # Importance Sampling estimate
h.x_IS</code></pre>
<pre><code>## [1] 0.0013578</code></pre>
<div id="we-can-see-that-it-is-pretty-close" class="section level4">
<h4>We can see that it is pretty close</h4>
<p><br />
</p>
</div>
</div>
<div id="sampling-error-of-the-is-estimate" class="section level3">
<h3>Sampling Error of the IS estimate</h3>
<pre class="r"><code>sd(h.x*w.x)</code></pre>
<pre><code>## [1] 0.003104949</code></pre>
<div id="we-can-see-that-the-sampling-error-is-very-small-which-is-an-indicator-of-a-good-estimate." class="section level4">
<h4>We can see that the sampling error is very small which is an indicator of a good estimate.</h4>
<p><br />
<br />
</p>
</div>
</div>
</div>
<div id="comparison-to-mc" class="section level2">
<h2>Comparison to MC</h2>
<p><br />
</p>
<div id="mc-caseii" class="section level3">
<h3>MC (CaseII)</h3>
<p><br />
</p>
</div>
<div id="direct-computation-2" class="section level3">
<h3>Direct Computation</h3>
<pre class="r"><code>N=10000  # Number of samples to extract
x=runif(N,0,3)  # Samples extracted from Unif(0,3)
g.x=1/sqrt(2*pi)*exp(-0.5*x^2)  # The g() ftn
1/2-3*mean(g.x)  # MC estimate</code></pre>
<pre><code>## [1] -0.001872618</code></pre>
<p><br />
</p>
</div>
<div id="sampling-error-of-the-mc-estimate" class="section level3">
<h3>Sampling Error of the MC estimate</h3>
<pre class="r"><code>sd(g.x)</code></pre>
<pre><code>## [1] 0.1393294</code></pre>
<div id="we-can-see-that-the-sampling-error-is-a-lot-bigger-than-is-estimates." class="section level4">
<h4>We can see that the sampling error is a lot bigger than IS estimate’s.</h4>
<p><br />
</p>
</div>
</div>
<div id="iteration-method-2" class="section level3">
<h3>Iteration Method</h3>
<pre class="r"><code>N=10000  # Number of samples to extract
x=rep(0,N)  # Storage for samples
g.x=rep(0,N)  # Storage for values of g()
mu.g.x=rep(0,N)  # Storage for values of mean of g()

for (i in 1:N){
  x[i]=runif(1,0,3)  # Extracting a sample
  g.x[i]=1/sqrt(2*pi)*exp(-1/2*x[i]^2)
  mu.g.x[i]=mean(g.x[1:i])
}

1/2-3*mu.g.x[N]  # MC estimate</code></pre>
<pre><code>## [1] -0.002913629</code></pre>
<p><br />
</p>
</div>
<div id="visualization-3" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>dev.new()
plot(1:N,0.5-3*mu.g.x,type=&quot;l&quot;, ylim=c(0,0.15))</code></pre>
<p><br />
</p>
</div>
<div id="sampling-error-of-the-mc-estimate-iteration-method" class="section level3">
<h3>Sampling Error of the MC estimate (Iteration method)</h3>
<pre class="r"><code>sd(g.x)</code></pre>
<pre><code>## [1] 0.1399884</code></pre>
<p><br />
</p>
</div>
<div id="mc-case-i" class="section level3">
<h3>MC (Case I)</h3>
<pre class="r"><code>N=100000  # Number of samples to extract
x=rnorm(N)  # Samples taken from N(0,1)
h.x=as.numeric(x&gt;=3)  # h() = P(X&gt;=3) ; X ~ N(0,1)
h.x_MC=mean(h.x)  # MC estimate
h.x_MC</code></pre>
<pre><code>## [1] 0.00141</code></pre>
<p><br />
</p>
</div>
<div id="sample-error-of-mc-case-i" class="section level3">
<h3>Sample Error of MC (Case I)</h3>
<pre class="r"><code>sd(h.x)</code></pre>
<pre><code>## [1] 0.03752367</code></pre>
<div id="case-i-has-a-better-sample-error-than-case-ii." class="section level4">
<h4>Case I has a better sample error than case II.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="markov-chain" class="section level2">
<h2>Markov Chain</h2>
<p><br />
</p>
<p><img src="images/IMG_0187.jpeg" /></p>
<p><br />
</p>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<pre class="r"><code>nsim=100
P=matrix(c(0.9,0.5,0.1,0.5),2,2)  # Transition matrix
x=matrix(c(0,1),nsim,2,byrow=T)  # Initial states

for(i in 1:(nsim-1)){
  x[i+1,]=x[i,]%*%P  # Matrix multiplication
}

dev.new()
plot(1:nsim,x[,1],type=&quot;l&quot;,ylim=c(0,1),col=&quot;red&quot;)
lines(1:nsim,x[,2],type=&quot;l&quot;,col=&quot;blue&quot;)
legend(&quot;topright&quot;, legend=c(&quot;Sunny&quot;, &quot;Rainy&quot;), col=c(&quot;red&quot;, &quot;blue&quot;), lty=1, cex=0.8)</code></pre>
<div id="we-can-see-that-it-converges." class="section level4">
<h4>We can see that it converges.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="gibbs-sampler-one-of-mcmc-methods" class="section level2">
<h2>Gibbs Sampler (One of MCMC methods)</h2>
<p><br />
</p>
<p><img src="images/IMG_0189.jpeg" /></p>
<p><br />
</p>
<pre class="r"><code>N=10000  # Total number of iterations(time)
burn=N/2  # Burn-in period is half of N I guess

# data (a single obs.)
y1=0
y2=0

# correlation
rho=0.8

# initial values
theta.1=rep(1,N)  # Starting point is 1 for theta1
theta.2=rep(-10,N)  # Starting point is -10 for theta2
theta=cbind(theta.1,theta.2)  # Theta binded

# iterations
for (i in 1:(N-1)){
  # Taking samples from the conditional dstns of theta1 and theta2 respectively
  theta.1[i+1]=rnorm(1,y1+rho*(theta.2[i]-y2),sqrt(1-rho^2))
  theta.2[i+1]=rnorm(1,y2+rho*(theta.1[i+1]-y1),sqrt(1-rho^2))
  # Samples binded
  theta[i+1,]=c(theta.1[i+1],theta.2[i+1])
}

head(theta)</code></pre>
<pre><code>##         theta.1     theta.2
## [1,]  1.0000000 -10.0000000
## [2,] -7.7187731  -6.1323402
## [3,] -3.9589455  -2.6244160
## [4,] -1.5296570  -0.3560851
## [5,]  0.8830352   0.8126232
## [6,]  1.3681496   1.0873443</code></pre>
<p><br />
</p>
<div id="visualizations" class="section level3">
<h3>Visualizations</h3>
<pre class="r"><code># check traceplots
dev.new()
par(mfrow=c(2,1))
plot(1:N,theta.1,type=&quot;l&quot;)
plot(1:N,theta.2,type=&quot;l&quot;)</code></pre>
<div id="the-values-of-theta1-and-theta2-converge." class="section level4">
<h4>The values of theta1 and theta2 converge.</h4>
<p><br />
</p>
<pre class="r"><code>dev.new()
par(mfrow=c(1,1))
plot(theta)</code></pre>
</div>
<div id="a-simple-visualization." class="section level4">
<h4>A simple visualization.</h4>
<p><br />
</p>
<pre class="r"><code>ntr=1000 
dev.new()
par(mfrow=c(1,1))
plot(theta.1[1:ntr],theta.2[1:ntr],type=&quot;l&quot;)
text(theta.1[1:ntr],theta.2[1:ntr],labels=1:10,cex=1, font=5)</code></pre>
</div>
<div id="with-this-plot-we-can-see-the-movement-of-the-convergence-i-guess." class="section level4">
<h4>With this plot, we can see the movement of the convergence I guess.</h4>
<p><br />
</p>
</div>
</div>
<div id="the-averages-of-thetas-these-are-the-estimates" class="section level3">
<h3>The averages of thetas: these are the estimates</h3>
<pre class="r"><code>colMeans(theta)</code></pre>
<pre><code>##      theta.1      theta.2 
##  0.003536145 -0.001089851</code></pre>
<p><br />
</p>
</div>
<div id="the-correlation-of-thetas-from-the-samples-using-conditional-dstns" class="section level3">
<h3>The correlation of thetas from the samples using conditional dstns</h3>
<pre class="r"><code>cor(theta)</code></pre>
<pre><code>##           theta.1   theta.2
## theta.1 1.0000000 0.7914565
## theta.2 0.7914565 1.0000000</code></pre>
<p><br />
</p>
</div>
<div id="the-correlation-of-thetas-without-the-burn-in-period" class="section level3">
<h3>The correlation of thetas without the Burn-in period</h3>
<pre class="r"><code>cor(theta[burn:N,])</code></pre>
<pre><code>##           theta.1   theta.2
## theta.1 1.0000000 0.7888511
## theta.2 0.7888511 1.0000000</code></pre>
<div id="excluding-the-burn-in-period-might-and-might-not-improve-i-guess." class="section level4">
<h4>Excluding the Burn-in period might and might not improve, I guess.</h4>
<p><br />
</p>
</div>
</div>
<div id="the-actual-dstn" class="section level3">
<h3>The actual dstn</h3>
<pre class="r"><code>library(mvtnorm)  # To sample from an actual BVN dstn

joint.s=rmvnorm(N/2,mean=c(y1,y2),sig=matrix(c(1,rho,rho,1),2,2))  # The actual dstn

head(joint.s)</code></pre>
<pre><code>##            [,1]         [,2]
## [1,]  1.1671424  0.496836150
## [2,]  0.5041658  1.122542636
## [3,] -0.4558893  0.007587043
## [4,]  0.1457435  0.901070714
## [5,] -1.7747480 -1.171497320
## [6,]  0.2304940  0.237404692</code></pre>
</div>
<div id="visualization-4" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>plot(xlim=c(-10,4), ylim=c(-10,4), joint.s)</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
</div>
<div id="the-averages-of-thetas-these-are-the-estimates-1" class="section level3">
<h3>The averages of thetas : these are the estimates</h3>
<pre class="r"><code>colMeans(joint.s)</code></pre>
<pre><code>## [1] -0.01514000 -0.00441913</code></pre>
</div>
<div id="the-correlation-of-thetas-from-the-samples-using-the-actual-dstn" class="section level3">
<h3>The correlation of thetas from the samples using the actual dstn</h3>
<pre class="r"><code>cor(joint.s)</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 1.0000000 0.8031811
## [2,] 0.8031811 1.0000000</code></pre>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="metropolis-algorithm" class="section level2">
<h2>Metropolis algorithm</h2>
<p><br />
</p>
<p><img src="images/IMG_0191.jpg" /></p>
<p><br />
</p>
<pre class="r"><code>N=10000
burn.in=N/2  # Burn-in period is half of N I guess

# data (a single obs.)
y1=0
y2=0

# correlation
rho=0.8

# initial value
sig2=1-rho^2    # variance for posterior density
sig2.prop=0.1   # variance for proposal density
theta.1=rep(1,N)  # Initial value is 1 I guess
theta.2=rep(1,N)


# iterations
for (i in 1:(N-1)){
  
  # for theta.1
  theta.1.star=rnorm(1,theta.1[i],sqrt(sig2.prop))  # Sample from the proposal dstn
  U.1=runif(1,0,1)  # U: auxiliary variable
  
  # Ratio of the two densities
  ratio.1=dnorm(theta.1.star,y1+rho*(theta.2[i]-y2),1-rho^2)/dnorm(theta.1[i],y1+rho*(theta.2[i]-y2),1-rho^2)
  
  # Probability of move
  alpha.1=min(ratio.1,1)
  
  if (U.1&lt;=alpha.1){theta.1[i+1]=theta.1.star}  # Accepted
  else theta.1[i+1]=theta.1[i]
  
  
  ## for theta.2
  theta.2.star=rnorm(1,theta.2[i],sqrt(sig2.prop))
  U.2=runif(1,0,1)
  
  # Ratio of the two densities
  ratio.2=dnorm(theta.2.star,y2+rho*(theta.1[i+1]-y1),1-rho^2)/dnorm(theta.2[i],y2+rho*(theta.1[i+1]-y1),1-rho^2)
  
  # Probability of move
  alpha.2=min(ratio.2,1)
  
  if (U.2&lt;=alpha.2){theta.2[i+1]=theta.2.star}  # Accepted
  else theta.2[i+1]=theta.2[i]
  
}

theta=cbind(theta.1,theta.2)  # Binding the thetas together

head(theta)</code></pre>
<pre><code>##        theta.1   theta.2
## [1,] 1.0000000 1.0000000
## [2,] 0.8832225 1.0000000
## [3,] 1.2421016 1.0292891
## [4,] 1.2421016 1.1380647
## [5,] 1.3794453 1.1380647
## [6,] 0.7972057 0.8701732</code></pre>
<p><br />
</p>
<div id="visualizations-1" class="section level3">
<h3>Visualizations</h3>
<pre class="r"><code># traceplots
dev.new()
par(mfrow=c(2,1))
plot(1:N,theta.1,type=&quot;l&quot;)
plot(1:N,theta.2,type=&quot;l&quot;)</code></pre>
<p><br />
</p>
<pre class="r"><code>dev.new()
par(mfrow=c(1,2))
hist(theta[(burn.in+1):N,1])
hist(theta[(burn.in+1):N,1])</code></pre>
<p><br />
</p>
<pre class="r"><code>dev.new()
par(mfrow=c(1,1))
plot(theta[(burn.in+1):N,])</code></pre>
<p><br />
</p>
</div>
<div id="the-averages-of-theta1-and-theta2-these-are-the-estimates" class="section level3">
<h3>The averages of theta1 and theta2: These are the estimates</h3>
<pre class="r"><code>colMeans(theta[(burn.in+1):N,])</code></pre>
<pre><code>##    theta.1    theta.2 
## 0.07564554 0.06914730</code></pre>
<p><br />
</p>
</div>
<div id="the-correlation-of-theta1-and-theta2" class="section level3">
<h3>The correlation of theta1 and theta2</h3>
<pre class="r"><code>cor(theta[(burn.in+1):N,])</code></pre>
<pre><code>##           theta.1   theta.2
## theta.1 1.0000000 0.7980239
## theta.2 0.7980239 1.0000000</code></pre>
<p><br />
<br />
<br />
</p>
<p><img src="images/IMG_0192.jpeg" /></p>
<p><br />
</p>
</div>
<div id="the-metropolis-algorithm-made-into-a-function-because-we-need-to-run-for-4-different-variances" class="section level3">
<h3>The metropolis algorithm made into a function (because we need to run for 4 different variances)</h3>
<pre class="r"><code>mh=function(v,sigma,x0,N){
  x=numeric(N)  # Making N number of 0s
  x[1]=x0  # Starting point
  k=0
  for (i in 2:N){  # Starts at 2 because 1 is the starting point
    y=rnorm(1,x[i-1],sigma)  # Sampling the candidate from the proposal dstn with 
                             # mean: previous value and variance: sigma
    u=runif(1)  # The auxiliary variable
    alpha=min(dt(y,v)/dt(x[i-1],v),1)  # Calculating the probability of move
                             # The ratio r is p(candidate|y)/p(previous value|y)
                             # Where p() is in this case the t dstn with df=v
    if (u&lt;=alpha){x[i]=y}  # Accepting the candidate
    else {x[i]=x[i-1]
    k=k+1  # Counting the number of rejections
    }
  }
  return(list(x=x,k=k))
}</code></pre>
<p><br />
</p>
</div>
<div id="lets-run-the-algorithm" class="section level3">
<h3>Let’s run the algorithm</h3>
<pre class="r"><code>N=1000  # Number of samples
burn.in=N/2  # The Burn-in period
v=4  # df of t dstn
x0=1  # Starting point
sigma=c(0.05,0.5,2,16)  # Different variances for proposal dstns

mh1=mh(v,sigma[1],x0,N)
mh2=mh(v,sigma[2],x0,N)
mh3=mh(v,sigma[3],x0,N)
mh4=mh(v,sigma[4],x0,N)</code></pre>
<p><br />
</p>
</div>
<div id="the-number-of-candidates-rejected-by-each-variance" class="section level3">
<h3>The number of candidates rejected by each variance</h3>
<pre class="r"><code>c(mh1$k,mh2$k,mh3$k,mh4$k)/N</code></pre>
<pre><code>## [1] 0.020 0.146 0.455 0.908</code></pre>
<div id="we-can-see-that-when-the-variancelow-few-rejected-variancehigh-many-rejected." class="section level4">
<h4>We can see that, when the variance(low) =&gt; Few rejected, variance(high) =&gt; Many rejected.</h4>
<p><br />
</p>
</div>
</div>
<div id="visualizations-2" class="section level3">
<h3>Visualizations</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(2,2))
plot(1:N,mh1$x,type=&quot;l&quot;)  # The trace plots
plot(1:N,mh2$x,type=&quot;l&quot;)
plot(1:N,mh3$x,type=&quot;l&quot;)
plot(1:N,mh4$x,type=&quot;l&quot;)</code></pre>
<div id="the-frequent-horizontal-lines-in-the-trace-plot-means-that-there-were-many-rejections.-horizontal-line-means-that-previous-values-were-used-many-times-because-candidates-were-rejected." class="section level4">
<h4>The frequent horizontal lines in the trace plot means that there were many rejections. Horizontal line means that previous values were used many times because candidates were rejected.</h4>
<p><br />
</p>
<pre class="r"><code>a=seq(-5,5,by=0.01)  # To plot the t dstn

dev.new()
par(mfrow=c(2,2))

hist(mh1$x[(burn.in+1):N],freq=F,ylim=c(0,0.7))  # Histogram of the samples
                                                 # without the Burn-in period
lines(a,dt(a,df=4))  # t dstn with df=4: The actual dstn

hist(mh2$x[(burn.in+1):N],freq=F,ylim=c(0,0.4))
lines(a,dt(a,df=4))

hist(mh3$x[(burn.in+1):N],freq=F,ylim=c(0,0.4))
lines(a,dt(a,df=4))

hist(mh4$x[(burn.in+1):N],freq=F,ylim=c(0,0.4))
lines(a,dt(a,df=4))</code></pre>
</div>
<div id="we-can-see-that-when-the-variance-is-too-low-or-high-the-dstn-of-the-samples-were-not-approximate-to-the-actual-dstn.-whereas-when-the-variance-is-modest-the-samples-were-pretty-approximate-to-the-actual-dstn." class="section level4">
<h4>We can see that when the variance is too low or high, the dstn of the samples were not approximate to the actual dstn. Whereas when the variance is modest, the samples were pretty approximate to the actual dstn.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="metropolis-hastings-algorithm" class="section level2">
<h2>Metropolis-Hastings algorithm</h2>
<p><br />
</p>
<p><img src="images/IMG_0193.jpeg" /></p>
<p><br />
</p>
<div id="the-m-h-algorithm" class="section level3">
<h3>The M-H algorithm</h3>
<pre class="r"><code>N=10000  # Number of samples
burn.in=N/2  # The Burn-in period
sigma=4  # of the Rayleigh dstn
x=numeric(N)  # Initializing the spaces for the chain
x[1]=rchisq(1,df=1)  # Starting point
k=0  # Number of rejected candidates

for (i in 2:N){  # Starts from 2 because 1 is the starting point
  x.star=rchisq(1,df=x[i-1])  # Candidate taken from chisq dstn with df=previous value
  u=runif(1)  # Auxiliary variable
  
  # For calculating the ratio r
  num=(x.star/(sigma^2)*exp(-x.star^2/(2*sigma^2)))/dchisq(x.star,df=x[i-1])
  den=(x[i-1]/(sigma^2)*exp(-x[i-1]^2/(2*sigma^2)))/dchisq(x[i-1],df=x.star)
  
  alpha=min(num/den,1)  # The probability of move
  if (u&lt;=alpha){x[i]=x.star}  # Accepting the candidate
  else {x[i]=x[i-1]
  k=k+1 # Counting the number of rejected candidates
  }
}</code></pre>
<p><br />
</p>
</div>
<div id="visualizations-3" class="section level3">
<h3>Visualizations</h3>
<pre class="r"><code>dev.new()
plot(1:N,x,type=&quot;l&quot;)  # The trace plot</code></pre>
<p><br />
</p>
<pre class="r"><code># install.packages(&quot;VGAM&quot;)
library(VGAM) # For drayleigh()</code></pre>
<pre><code>## Loading required package: stats4</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## 
## Attaching package: &#39;VGAM&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:coda&#39;:
## 
##     nvar</code></pre>
<pre class="r"><code>xgrid=seq(0,20,by=0.1)
ytrue=drayleigh(xgrid,scale=sigma)  # The density of the actual dstn

dev.new()
hist(x[(burn.in+1):N],freq=F, main=&quot;Comparison to the actual dstn&quot;)  # freq=F: to display density on y axis not the frequency
lines(xgrid,ytrue,type=&quot;l&quot;)  # The actual dstn</code></pre>
<div id="we-can-see-that-the-samplesthe-histogram-is-pretty-approximate-to-the-actual-dstnthe-line." class="section level4">
<h4>We can see that the samples(the histogram) is pretty approximate to the actual dstn(the line).</h4>
<p><br />
<br />
<br />
</p>
<p><img src="images/IMG_0194.jpeg" /></p>
<p><br />
</p>
<pre class="r"><code>N=100000  # Number of samples
burn.in=N/2  # Burn-in period

## initial value
sig2=100  # variance of the proposal dstn
theta=rep(0,N)  # Initializing the space for theta

p.th=function(th){   # The actual dstn
  0.3*exp(-0.2*th^2)+0.7*exp(-0.2*(th-10)^2)
}

## iterations
for (i in 1:(N-1)){  # No value for starting point was given so I guess it&#39;s 0
  
  theta.star=rnorm(1,theta[i],sqrt(sig2))  # candidate
  U=runif(1,0,1)  # auxiliary variable
  
  ratio=p.th(theta.star)/p.th(theta[i])  # the ratio r
  # Note that since the proposal dstn is symmetric, the ratio r is just like Metropolis&#39;
  
  alpha=min(ratio,1)  # The probability of move
  
  if (U&lt;=alpha){theta[i+1]=theta.star}  # Accepting the candidate
  else theta[i+1]=theta[i]
  
}</code></pre>
<p><br />
</p>
</div>
</div>
<div id="visualizations-4" class="section level3">
<h3>Visualizations</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(1,1))
plot(burn.in+1:N,theta,type=&quot;l&quot;)  # the trace plot</code></pre>
<p><br />
</p>
<pre class="r"><code>th.grid=seq(-10,20,by=1)
th.true=rep(0,length(th.grid))
for (i in 1:length(th.grid)){
  th.true[i]=p.th(th.grid[i])
}
th.true=th.true/sum(th.true)  # Normalizing I guess? This makes sum(th.true) = 1

dev.new()
par(mfrow=c(1,1))
hist(theta,freq=FALSE, main=&quot;Comparison to the actual dstn&quot;)
lines(th.grid,th.true,type=&quot;l&quot;)</code></pre>
<p><br />
</p>
</div>
<div id="estimate-for-the-mean-of-the-target-dstn-i-guess" class="section level3">
<h3>Estimate for the mean of the target dstn I guess?</h3>
<pre class="r"><code>mean(theta[(burn.in+1):N])</code></pre>
<pre><code>## [1] 7.029065</code></pre>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="convergence-diagnostics" class="section level2">
<h2>Convergence Diagnostics</h2>
<p><br />
</p>
<p><img src="images/ss2.png" /></p>
<p><img src="images/IMG_0195.jpeg" /></p>
<p><br />
</p>
<pre class="r"><code># install.packages(&quot;geoR&quot;)
library(coda)  # Our main convergence diagnostic package
library(geoR)</code></pre>
<pre><code>## --------------------------------------------------------------
##  Analysis of Geostatistical Data
##  For an Introduction to geoR go to http://www.leg.ufpr.br/geoR
##  geoR version 1.8-1 (built on 2020-02-08) is now loaded
## --------------------------------------------------------------</code></pre>
<pre class="r"><code>library(MCMCpack)</code></pre>
<p><br />
</p>
<div id="running-metropolis-algorithm" class="section level3">
<h3>Running Metropolis algorithm</h3>
<pre class="r"><code>library(mvtnorm)

N=10000
burn.in=N/2

# data
y=c(0,0)

# correlation
rho=0.8

# initial value
cov.var=matrix(c(1,rho,rho,1),2,2)        # cov for posterior density
cov.var.prop=matrix(c(1,rho,rho,1),2,2)   # cov for proposal density
theta=matrix(0,nrow=N,ncol=2)

# iterations
for (i in 1:(N-1)){
  theta.star=rmvnorm(1,mean=theta[i,],sigma=cov.var.prop)  # Candidate
  U=runif(1,0,1)  # Auxiliary variable
  
  # The ratio r
  ratio=dmvnorm(theta.star,mean=y,sigma=cov.var)/dmvnorm(theta[i,],mean=y,sigma=cov.var)
  
  # The probability of move
  alpha=min(ratio,1)
  
  if (U&lt;=alpha){theta[i+1,]=theta.star}  # Accepting the candidate
  else theta[i+1,]=theta[i,]
}</code></pre>
<p><br />
</p>
</div>
<div id="visual-inspection-traceplot" class="section level3">
<h3>Visual inspection: Traceplot</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(2,1))
plot(1:N,theta[,1],type=&quot;l&quot;)
plot(1:N,theta[,2],type=&quot;l&quot;)</code></pre>
<div id="we-can-see-that-the-chains-dont-get-stuck-in-certain-areas.-so-these-are-kind-of-good-i-guess." class="section level4">
<h4>We can see that the chains don’t get stuck in certain areas. So these are kind of good I guess.</h4>
<p><br />
</p>
</div>
</div>
<div id="visual-inspection-running-mean-plot" class="section level3">
<h3>Visual inspection: Running mean plot</h3>
<pre class="r"><code>mean.th1=rep(0,N)  # To store the mean values
mean.th2=rep(0,N)  # To store the mean values

# Get the mean of the draws UP TO EACH ITERATION
for (i in 1:N){
  mean.th1[i]=mean(theta[1:i,1])
  mean.th2[i]=mean(theta[1:i,2])
}

dev.new()
par(mfrow=c(2,1))
plot(1:N,mean.th1,type=&quot;l&quot;)
plot(1:N,mean.th2,type=&quot;l&quot;)</code></pre>
<div id="both-thetas-means-seem-to-converge." class="section level4">
<h4>Both thetas’ means seem to converge.</h4>
<p><br />
<br />
</p>
</div>
</div>
<div id="convergence-diagnostic-using-coda-package" class="section level3">
<h3>Convergence diagnostic using ‘coda’ package</h3>
<p><br />
</p>
<div id="before-we-use-the-diagnostics-we-must-turn-our-chains-into-mcmc-objects." class="section level4">
<h4>Before we use the diagnostics, we must TURN OUR CHAINS INTO MCMC objects.</h4>
<pre class="r"><code># mcmc() is from &#39;coda&#39; package
theta=mcmc(theta)  # We must turn our chains into MCMC objects

summary(theta)  # Summary statistics for the distribution</code></pre>
<pre><code>## 
## Iterations = 1:10000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean     SD Naive SE Time-series SE
## [1,] -0.0004794 0.9786 0.009786        0.03069
## [2,]  0.0137367 0.9673 0.009673        0.02970
## 
## 2. Quantiles for each variable:
## 
##        2.5%     25%       50%    75% 97.5%
## var1 -1.915 -0.6659 -0.021271 0.6639 1.920
## var2 -1.803 -0.6419 -0.004077 0.6589 1.931</code></pre>
</div>
<div id="thinning-interval-here-means-the-keeping-every-kth-samples-only-in-the-traceplot." class="section level4">
<h4>Thinning interval here means the ‘keeping every kth samples only’ in the traceplot.</h4>
<p><br />
</p>
</div>
</div>
<div id="visual-inspection-using-coda-package" class="section level3">
<h3>Visual inspection using ‘coda’ package</h3>
<pre class="r"><code>plot(theta)  # This gives us both dense plot and traceplot</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<pre class="r"><code># densplot(theta)
# traceplot(theta)</code></pre>
<div id="note-that-these-plots-are-plotted-against-the-variance-of-theta-instead-of-the-values-of-theta-directly." class="section level4">
<h4>Note that these plots are plotted against the variance of theta instead of the values of theta directly.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="gelman-and-rubin-diagnostic" class="section level2">
<h2>Gelman and Rubin Diagnostic</h2>
<p><br />
</p>
<p><img src="images/IMG_0296.jpeg" /></p>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
<div id="adaptive-simulation-algorithm" class="section level2">
<h2>Adaptive Simulation algorithm</h2>
<p><br />
</p>
<p><img src="images/IMG_0301.jpeg" /></p>
<p><br />
</p>
<div id="lets-monitor-the-acceptance-rate-of-the-metropolis-algo.-using-adaptive-simulation-algo." class="section level4">
<h4>Let’s monitor the acceptance rate of the Metropolis algo. using adaptive simulation algo.</h4>
<p><br />
</p>
<pre class="r"><code>library(mvtnorm)  # for mvnorm

N=10000  # Increase iterations for better performance
burn.in=N/2

## data
y=c(0,0)

## correlation
rho=0.8
c=2.4/sqrt(2)  # d=2 since it&#39;s a bivariate model

## initial value
cov.var=matrix(c(1,rho,rho,1),2,2)        #cov for posterior density
cov.var.prop=matrix(c(1,rho,rho,1),2,2)   #cov for proposal density
theta=matrix(0,nrow=N,ncol=2)

theta.1_out=rep(1,N)
c_out=NULL

## iterations
for (i in 1:(N-1)){
  
  # The adaptive simulation algo. part
  if (i&gt;1 &amp; ((i-1)%%100==0)){  # %% gets the remainder
                               # so this is saying every 100 iterations
    ar=sum(diff(theta.1_out[(i-100):(i-1)])!=0)/99
    # ar is the sum of the nonzeros of the sequence&#39;s differences?
    
    if (ar&lt;0.44){c=c/sqrt(2)}  # we need smaller variance
    else if (ar&gt;0.44){c=c*sqrt(2)}  # we need larger variance
  }
  
  
  # The Metropolis algo. part
  theta.star=rmvnorm(1,mean=theta[i,],sigma=c^2*cov.var.prop)
  # taking the candidate from the proposal dstn
  # c: scaling factor
  
  U=runif(1,0,1)  
  
  ratio=dmvnorm(theta.star,mean=y,sigma=cov.var)/dmvnorm(theta[i,],mean=y,sigma=cov.var)
  alpha=min(ratio,1)
  
  if (U&lt;=alpha){theta[i+1,]=theta.star}
  else theta[i+1,]=theta[i,]
  
  theta.1_out[i]=theta[i,1]
  c_out[i]=c
}</code></pre>
<p><br />
</p>
</div>
<div id="visualization-5" class="section level3">
<h3>Visualization</h3>
<p><br />
</p>
<pre class="r"><code>dev.new()
par(mfrow=c(2,1))
plot(1:N,theta[,1],type=&quot;l&quot;)
plot(1:N,theta[,2],type=&quot;l&quot;)</code></pre>
<p><br />
</p>
<pre class="r"><code>dev.new()
par(mfrow=c(1,3))
plot(theta[,1],theta[,2])
hist(theta[,1])
hist(theta[,2])</code></pre>
<p><br />
</p>
</div>
<div id="estimated-means-of-theta1-and-theta2" class="section level3">
<h3>Estimated means of theta1 and theta2</h3>
<pre class="r"><code>mean(theta[(burn.in+1):N,1])</code></pre>
<pre><code>## [1] 0.0007269106</code></pre>
<pre class="r"><code>mean(theta[(burn.in+1):N,2])</code></pre>
<pre><code>## [1] 0.04818496</code></pre>
<div id="both-are-pretty-close-to-0." class="section level4">
<h4>Both are pretty close to 0.</h4>
<p><br />
</p>
</div>
</div>
<div id="the-acceptance-rates-after-iterations" class="section level3">
<h3>The acceptance rates after iterations</h3>
<pre class="r"><code>library(coda)
library(MCMCpack)

theta=mcmc(theta)  # we need to turn it into an mcmc object first
1-rejectionRate(theta) # The acceptance rate</code></pre>
<pre><code>##      var1      var2 
## 0.4417442 0.4417442</code></pre>
<div id="both-are-pretty-close-to-0.44-which-is-great-i-guess." class="section level4">
<h4>Both are pretty close to 0.44 which is great I guess.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="adaptive-metropolis-algorithm" class="section level2">
<h2>Adaptive Metropolis algorithm</h2>
<p><br />
</p>
<p><img src="images/IMG_0299.jpeg" /></p>
<div id="ignore-the-change-the-from-the-image" class="section level3">
<h3>Ignore the “Change the ~” from the image</h3>
<p><br />
</p>
</div>
<div id="also-i-guess-you-have-to-use-conditional-post.-dstn-in-this-case" class="section level3">
<h3>Also, I guess you have to use conditional post. dstn in this case?</h3>
<p><br />
</p>
<pre class="r"><code>N=10000
burn.in=N/2

## data
y=c(0,0)

## correlation
rho=0.8

## scaling factor
c1=2.4/sqrt(1)
c2=2.4/sqrt(1)
eps=0.01  # This is to prevent the variance from shrinking to zero

## initial value
sig2=1-rho^2                   # variance for posterior density
var1.prop=1                    # variance for proposal density
var2.prop=1                    # variance for proposal density

theta.1=rep(10,N)
theta.2=rep(0,N)

c1_out=NULL
c2_out=NULL


#### iterations
for (i in 1:(N-1)){
  
  # Adaptive algorithm part
  
  ## update variance of proposal
  if (i&gt;100){
    var1.prop=var(theta.1[1:(i-1)])  
    var2.prop=var(theta.2[1:(i-1)])  
  }
  
  ## control acceptance rate every 100 iterations
  if (i&gt;1 &amp; ((i-1)%%100==0)){  
    ar1=sum(diff(theta.1[(i-100):(i-1)])!=0)/99
    if (ar1&lt;0.44){c1=c1/sqrt(2)}
    else if (ar1&gt;0.44){c1=c1*sqrt(2)}
    
    ar2=sum(diff(theta.2[(i-100):(i-1)])!=0)/99
    if (ar2&lt;0.44){c2=c2/sqrt(2)}
    else if (ar2&gt;0.44){c2=c2*sqrt(2)}
  }
  
  
  # Metropolis part
  
  ## for theta.1
  theta.1.star=rnorm(1,theta.1[i],c1^2*var1.prop+c1^2*eps)
  U.1=runif(1,0,1)
  
  ratio.1=dnorm(theta.1.star,y[1]+rho*(theta.2[i]-y[2]),sqrt(sig2))/dnorm(theta.1[i],y[1]+rho*(theta.2[i]-y[2]),sqrt(sig2))
  alpha.1=min(ratio.1,1)
  
  if (U.1&lt;=alpha.1){theta.1[i+1]=theta.1.star} else theta.1[i+1]=theta.1[i]
  
  ## for theta.2
  theta.2.star=rnorm(1,theta.2[i],c2^2*var2.prop+c2^2*eps)
  U.2=runif(1,0,1)
  
  ratio.2=dnorm(theta.2.star,y[2]+rho*(theta.1[i]-y[1]),sqrt(sig2))/dnorm(theta.2[i],y[2]+rho*(theta.1[i]-y[1]),sqrt(sig2))
  alpha.2=min(ratio.2,1)
  
  if (U.2&lt;=alpha.2){theta.2[i+1]=theta.2.star} else theta.2[i+1]=theta.2[i]
  
  c1_out[i]=c1
  c2_out[i]=c2
}</code></pre>
<p><br />
</p>
</div>
<div id="visualizations-5" class="section level3">
<h3>Visualizations</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(2,1))
plot(1:N,theta.1,type=&quot;l&quot;)
plot(1:N,theta.2,type=&quot;l&quot;)</code></pre>
<p><br />
</p>
<pre class="r"><code>par(mfrow=c(1,3))
plot(theta.1,theta.2)
hist(theta.1)
hist(theta.2)</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p><br />
</p>
</div>
<div id="estimated-means-of-theta1-and-theta2-1" class="section level3">
<h3>Estimated means of theta1 and theta2</h3>
<pre class="r"><code>mean(theta.1[(burn.in+1):N])</code></pre>
<pre><code>## [1] 0.02989819</code></pre>
<pre class="r"><code>mean(theta.2[(burn.in+1):N])</code></pre>
<pre><code>## [1] 0.02687757</code></pre>
<div id="both-are-pretty-close-to-0" class="section level4">
<h4>Both are pretty close to 0</h4>
<p><br />
</p>
</div>
</div>
<div id="acceptance-rate-after-iterations" class="section level3">
<h3>Acceptance rate after iterations</h3>
<pre class="r"><code>library(coda)
library(MCMCpack)

theta.1=mcmc(theta.1)
theta.2=mcmc(theta.2)
1-rejectionRate(theta.1)</code></pre>
<pre><code>##      var1 
## 0.4389439</code></pre>
<pre class="r"><code>1-rejectionRate(theta.2)</code></pre>
<pre><code>##      var1 
## 0.4221422</code></pre>
<div id="both-have-acceptance-rates-pretty-close-to-0.44" class="section level4">
<h4>Both have acceptance rates pretty close to 0.44</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="slice-sampler" class="section level2">
<h2>Slice Sampler</h2>
<p><br />
</p>
<p><img src="images/IMG_0300.jpeg" /></p>
<p><br />
</p>
<pre class="r"><code>nsim=100000  # Increase iterations for better performance

u=rep(1,nsim)
x=rep(1,nsim)

for(i in 2:nsim){
  u[i]&lt;-runif(1,0,0.5*exp(-sqrt(x[i-1])))
  x[i]&lt;-runif(1,0,(log(2*u[i]))^2)
}</code></pre>
<p><br />
</p>
<pre class="r"><code>dev.new()
par(mfrow=c(1,1))

a=seq(0,50,0.01)
fa=0.5*exp(-sqrt(a))

hist(x,freq=F,xlim=c(0,50),breaks=200)
lines(a,fa,type=&quot;l&quot;,col=&quot;red&quot;)</code></pre>
<div id="the-red-line-is-the-actual-density-and-the-histogram-is-of-samples-from-the-slice-sampler.-its-pretty-close-indeed." class="section level4">
<h4>The red line is the actual density, and the histogram is of samples from the slice sampler. It’s pretty close indeed.</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
<div id="bayesian-linear-regression-model" class="section level2">
<h2>Bayesian Linear Regression model</h2>
<p><br />
</p>
</div>
<div id="example-1" class="section level2">
<h2>Example 1</h2>
<p><img src="images/IMG_0377.jpeg" /></p>
<p><img src="images/IMG_0378.jpeg" /></p>
<p><img src="images/IMG_0379.jpeg" /></p>
<p><br />
</p>
<div id="loading-data" class="section level3">
<h3>Loading data</h3>
<pre class="r"><code>library(LearnBayes)  # for the &#39;birdextinct&#39; data</code></pre>
<pre><code>## 
## Attaching package: &#39;LearnBayes&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:VGAM&#39;:
## 
##     laplace</code></pre>
<pre><code>## The following object is masked from &#39;package:MCMCpack&#39;:
## 
##     rdirichlet</code></pre>
<pre><code>## The following object is masked from &#39;package:TeachingDemos&#39;:
## 
##     triplot</code></pre>
<pre class="r"><code>data(&quot;birdextinct&quot;)
attach(birdextinct)

birdextinct</code></pre>
<pre><code>##                 species   time nesting size status
## 1           Sparrowhawk  3.030   1.000    0      1
## 2               Buzzard  5.464   2.000    0      1
## 3               Kestrel  4.098   1.210    0      1
## 4             Peregrine  1.681   1.125    0      1
## 5        Grey_partridge  8.850   5.167    0      1
## 6                 Quail  1.493   1.000    0      0
## 7  Red-legged_partridge  7.692   2.750    0      1
## 8              Pheasant  3.846   5.630    0      1
## 9            Water_rail 16.667   3.000    0      1
## 10            Corncrake  4.219   4.670    0      0
## 11              Moorhen  8.130   4.056    0      1
## 12                 Coot  5.000   1.000    0      1
## 13              Lapwing  7.299   6.960    0      0
## 14        Golden_plover  1.000   1.670    0      0
## 15        Ringed_plover 27.027   5.560    0      1
## 16               Curlew  3.106   2.830    0      0
## 17             Redshank  4.000   4.375    0      0
## 18                Snipe 16.129   4.125    0      0
## 19           Stock_dove  3.484   3.670    0      1
## 20            Rock_dove 37.037   8.330    0      1
## 21          Wood_pigeon  7.299   2.750    0      1
## 22               Cuckoo  2.525   1.430    0      0
## 23      Short-eared_owl  4.132   2.000    0      1
## 24           Little_owl  2.000   2.750    0      1
## 25               Magpie 10.000   4.500    0      1
## 26              Jackdaw  2.667   7.120    0      1
## 27         Carrion_crow  4.587   4.580    0      1
## 28                Raven 58.824   2.350    0      1
## 29              Skylark 32.258   6.870    1      1
## 30              Swallow  2.571   3.830    1      0
## 31         House_martin  2.160   5.000    1      0
## 32       Yellow_wagtail  1.000   1.250    1      0
## 33         Pied_wagtail  2.967   2.270    1      1
## 34         Meadow_pipit  9.524   5.350    1      1
## 35                 Wren 11.111   8.700    1      1
## 36              Dunnock  7.299   6.100    1      1
## 37                Robin  4.000   3.330    1      1
## 38            Stonechat  2.381   3.640    1      1
## 39             Wheatear  2.611   4.830    1      0
## 40            Blackbird  3.257   4.670    1      1
## 41          Song_thrush  1.701   1.700    1      1
## 42        Mistle_thrush  1.795   1.330    1      1
## 43  Grasshopper_warbler  1.198   1.000    1      0
## 44        Sedge_warbler  3.185   1.900    1      0
## 45          Whitethroat  2.273   4.420    1      0
## 46       Willow_warbler  1.111   1.250    1      0
## 47           Chiffchaff  1.000   1.000    1      0
## 48            Goldcrest  1.000   1.000    1      1
## 49   Spotted_flycatcher  1.230   1.000    1      0
## 50            Great_tit  6.061   2.500    1      1
## 51             Blue_tit  3.175   1.500    1      1
## 52         Yellowhammer  2.000   2.500    1      1
## 53         Reed_bunting  5.076   5.630    1      1
## 54            Chaffinch  1.934   2.370    1      1
## 55            Goldfinch  1.493   1.500    1      1
## 56              Redpoll  1.000   1.000    1      1
## 57               Linnet  5.102   6.500    1      1
## 58        House_sparrow  3.003   4.500    1      1
## 59         Tree_sparrow  1.898   2.170    1      1
## 60             Starling 41.667  11.620    1      1
## 61      Pied_flycatcher  1.000   1.000    1      0
## 62               Siskin  1.000   1.000    1      1</code></pre>
<div id="time-is-the-y-variable-and-nesting-size-and-status-are-the-x-variables.-note-that-size-and-status-are-binary." class="section level4">
<h4>time is the y variable, and nesting, size, and status are the x variables. Note that size and status are binary.</h4>
<p><br />
</p>
</div>
</div>
<div id="eda" class="section level3">
<h3>EDA</h3>
<pre class="r"><code>N &lt;- dim(birdextinct)[1]  # Sample size

logtime=log(time)  # time is very skewed and log makes it less skewed

dev.new()
par(mfrow=c(1,2))
hist(time)
hist(logtime)</code></pre>
<div id="since-time-is-very-skewed-we-used-log.-we-can-see-that-it-has-lessened-the-skewness-but-not-much-to-be-honest." class="section level4">
<h4>Since time is very skewed, we used log. We can see that it has lessened the skewness but not much to be honest.</h4>
<p><br />
</p>
</div>
</div>
<div id="frequentist-linear-regression" class="section level3">
<h3>Frequentist linear regression</h3>
<pre class="r"><code>lm.fit=lm(logtime~nesting+size+status,data=birdextinct,x=TRUE,y=TRUE)
summary(lm.fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = logtime ~ nesting + size + status, data = birdextinct, 
##     x = TRUE, y = TRUE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8410 -0.2932 -0.0709  0.2165  2.5167 
## 
## Coefficients:
##             Estimate Std. Error t value      Pr(&gt;|t|)    
## (Intercept)  0.43087    0.20706   2.081      0.041870 *  
## nesting      0.26501    0.03679   7.203 0.00000000133 ***
## size        -0.65220    0.16667  -3.913      0.000242 ***
## status       0.50417    0.18263   2.761      0.007712 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6524 on 58 degrees of freedom
## Multiple R-squared:  0.5982, Adjusted R-squared:  0.5775 
## F-statistic: 28.79 on 3 and 58 DF,  p-value: 0.00000000001577</code></pre>
<p><br />
</p>
</div>
<div id="residual-plot" class="section level3">
<h3>Residual plot</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(1,1))
plot(lm.fit$fitted.values,lm.fit$residuals,ylim=c(-1.5,1.5),xlab=&quot;Fitted Values&quot;,ylab=&quot;Residuals&quot;)
abline(h=0)
abline(h=2*summary(lm.fit)$sigma,lty=2)   
abline(h=-2*summary(lm.fit)$sigma,lty=2)
# sigma is the residual standard error (or residual standard deviation) which is a measure used to assess how well a linear regression model fits the data, smaller meaning the model fits the data better.</code></pre>
<p>There are no violations of equal variance of error terms, or the linearity of the model. Also there seem to be no extreme outliers although there is one that is outside of the 2 times the residual standard error.</p>
<p><br />
<br />
<br />
</p>
</div>
</div>
<div id="bayesian-linear-regression" class="section level2">
<h2>Bayesian linear regression</h2>
<p><br />
</p>
<div id="lets-use-blinreg-to-sample-from-the-joint-post.-dstn-of-beta-and-sigma_squared" class="section level4">
<h4>Let’s use blinreg() to sample from the joint post. dstn of beta and sigma_squared</h4>
<pre class="r"><code>theta.sample=blinreg(lm.fit$y,lm.fit$x,1000)

dev.new()
par(mfrow=c(2,2))
hist(theta.sample$beta[,2],main=&quot;Nesting&quot;,xlab=expression(beta[1]))
hist(theta.sample$beta[,3],main=&quot;Size&quot;,xlab=expression(beta[2]))
hist(theta.sample$beta[,4],main=&quot;Status&quot;,xlab=expression(beta[3]))
hist(theta.sample$sigma,main=&quot;Error SD&quot;,xlab=expression(sigma))</code></pre>
</div>
<div id="these-are-the-samples-from-marginal-posterior-distribution-of-each-variable.-error-sd-is-the-error-terms-calculated-from-the-samples.-they-look-like-normal-distribution-so-mean-and-median-are-pretty-much-the-same-so-we-can-choose-either." class="section level4">
<h4>These are the samples from marginal posterior distribution of each variable. Error SD is the error terms calculated from the samples(?). They look like normal distribution so mean and median are pretty much the same, so we can choose either.</h4>
<p><br />
</p>
</div>
<div id="comparison-of-coefficients" class="section level3">
<h3>Comparison of coefficients</h3>
<pre class="r"><code>apply(theta.sample$beta,2,quantile,c(0.025,0.5,0.975))  # 95% credible intervals</code></pre>
<pre><code>##       X(Intercept)  Xnesting      Xsize   Xstatus
## 2.5%  -0.003880442 0.1915111 -0.9530354 0.1448338
## 50%    0.429299768 0.2633242 -0.6435647 0.5291354
## 97.5%  0.854045611 0.3397266 -0.3169059 0.8546517</code></pre>
<pre class="r"><code># We can think the median(50%) as the bayes estimates of the coefficients since they seem like they follow normal distribution</code></pre>
<pre class="r"><code>lm.fit$coefficients  # freq&#39;s estimators of coefficients</code></pre>
<pre><code>## (Intercept)     nesting        size      status 
##   0.4308716   0.2650140  -0.6521982   0.5041655</code></pre>
<div id="we-can-see-that-both-freqs-and-bayes-results-are-similar.-this-is-due-to-bayes-linear-model-using-a-noninformative-prior.-bayes-provides-more-information-because-we-can-clearly-see-the-distribution-of-each-variables." class="section level4">
<h4>We can see that both freq’s and bayes’ results are similar. This is due to bayes linear model using a noninformative prior. Bayes provides more information because we can clearly see the distribution of each variables.</h4>
<p><br />
</p>
</div>
</div>
<div id="sigma-comparison" class="section level3">
<h3>Sigma comparison</h3>
<pre class="r"><code>quantile(theta.sample$sigma,c(0.025,0.5,0.975))  # 50% is the bayes estimate</code></pre>
<pre><code>##      2.5%       50%     97.5% 
## 0.5493634 0.6546392 0.7893300</code></pre>
<pre class="r"><code>summary(lm.fit)$sigma</code></pre>
<pre><code>## [1] 0.6524084</code></pre>
<div id="we-can-see-that-sigma-in-both-cases-are-similar-too." class="section level4">
<h4>We can see that sigma in both cases are similar too.</h4>
<p><br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="sampling-using-marginal-post.-dstn-of-sigma_squared" class="section level2">
<h2>Sampling using marginal post. dstn of sigma_squared</h2>
<p><br />
</p>
<div id="eda-1" class="section level3">
<h3>EDA</h3>
<pre class="r"><code>##Determine explanatory variables
X &lt;- as.matrix(birdextinct[,3:5])
cov.names &lt;- names(birdextinct[,c(-1,-2)])

##Pairs plot
pairs(cbind(logtime,X))</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<pre class="r"><code>##Add a column of 1s for the intercepts
X.int &lt;- cbind(rep(1,N),X)

##Determine the number of explanatory variables
k &lt;- dim(X.int)[2]</code></pre>
<p><br />
</p>
</div>
<div id="functions-to-use-to-sample-from-cond.-post.-dstn-of-beta-and-marginal-post.-dstn-of-sigma_squared" class="section level3">
<h3>Functions to use to sample from cond. post. dstn of beta and marginal post. dstn of sigma_squared</h3>
<pre class="r"><code>library(MASS)  # for mvrnorm

##Function for sampling from p(beta|sigma2,X,y) : cond. post. dstn of beta
sample.beta &lt;- function(sigma2,X,y){
  V.beta &lt;- solve(t(X)%*%X)             
  beta.hat &lt;- V.beta%*%t(X)%*%y
  return(mvrnorm(1,beta.hat,sigma2*V.beta))
}

#Function for sampling from p(sigma2|X,y) : marginal post. dstn of sigma_squared
sample.sigma2 &lt;- function(X,y,N,k){
  V.beta &lt;- solve(t(X)%*%X)            
  beta.hat &lt;- V.beta%*%t(X)%*%y
  s2 &lt;- t(y-X%*%beta.hat)%*%(y-X%*%beta.hat)/(N-k)
  return(1/rgamma(1,shape=(N-k)/2,rate=((N-k)/2)*s2))
}</code></pre>
<p><br />
</p>
</div>
<div id="sampling" class="section level3">
<h3>Sampling</h3>
<pre class="r"><code># Initialize variables for the posterior samples
nsim=1000
beta.post.samples = matrix(0,nsim,k)
sigma2.post.samples = rep(0,nsim)

# Iterations
for(i in 1:nsim){
  sigma2.post.samples[i] &lt;- sample.sigma2(X.int,logtime,N,k-1)
  beta.post.samples[i,] &lt;- sample.beta(sigma2.post.samples[i],X.int,logtime)
}</code></pre>
<p><br />
</p>
</div>
<div id="visualization-6" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>#Examine posterior samples
dev.new()
par(mfrow=c(3,2))
hist(sigma2.post.samples)
boxplot(as.data.frame(beta.post.samples[,-1]),names=cov.names)
abline(h=0)

plot(1:nsim,sigma2.post.samples,type=&quot;l&quot;)
plot(1:nsim,beta.post.samples[,2],type=&quot;l&quot;)
plot(1:nsim,beta.post.samples[,3],type=&quot;l&quot;)
plot(1:nsim,beta.post.samples[,4],type=&quot;l&quot;)</code></pre>
<div id="the-histogram-is-of-sigma_squared-and-the-boxplot-is-of-the-betas.-the-traceplot-is-to-show-the-convergence-of-the-samples-of-sigma_squared-and-each-beta." class="section level4">
<h4>The histogram is of sigma_squared, and the boxplot is of the betas. The traceplot is to show the convergence of the samples of sigma_squared and each beta.</h4>
<p><br />
</p>
</div>
</div>
<div id="the-bayes-estimates-of-sigma_squared" class="section level3">
<h3>The bayes estimates of sigma_squared</h3>
<pre class="r"><code>mean(sigma2.post.samples)</code></pre>
<pre><code>## [1] 0.4342814</code></pre>
</div>
<div id="the-bayes-estimates-of-betas" class="section level3">
<h3>The bayes estimates of betas</h3>
<pre class="r"><code>colMeans(beta.post.samples)</code></pre>
<pre><code>## [1]  0.4281336  0.2647335 -0.6567943  0.5072884</code></pre>
</div>
<div id="comparison-to-the-freqs-estimates" class="section level3">
<h3>Comparison to the freq’s estimates</h3>
<pre class="r"><code>lm.fit$coefficients  # The betas</code></pre>
<pre><code>## (Intercept)     nesting        size      status 
##   0.4308716   0.2650140  -0.6521982   0.5041655</code></pre>
<pre class="r"><code>(summary(lm.fit)$sigma)^2  # Sigma_squared</code></pre>
<pre><code>## [1] 0.4256367</code></pre>
<div id="we-can-see-that-both-bayes-and-freqs-results-are-similar.-we-can-say-that-out-bayes-model-is-doing-well-i-guess." class="section level4">
<h4>We can see that both bayes and freq’s results are similar. We can say that out bayes model is doing well I guess.</h4>
<p><br />
</p>
</div>
</div>
<div id="correlation-map-of-the-betas" class="section level3">
<h3>Correlation map of the betas</h3>
<pre class="r"><code>#Look at the correlation between the betas
pairs(beta.post.samples,labels=c(&quot;Intercept&quot;,cov.names))</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<p><br />
<br />
<br />
</p>
</div>
</div>
<div id="sampling-using-the-conditional-dstn-of-simga_squared-gibbs-sampler-i-guess" class="section level2">
<h2>Sampling using the conditional dstn of simga_squared (Gibbs sampler I guess)</h2>
<p><br />
</p>
<div id="functions-to-sample-sigma_squared-from-the-cond.-post.-dstn-of-sigma_squared-and-beta-from-the-cond.-post.-dstn-of-beta-same-one-as-above" class="section level3">
<h3>Functions to sample sigma_squared from the cond. post. dstn of sigma_squared and beta from the cond. post. dstn of beta (same one as above)</h3>
<pre class="r"><code>##Function for sampling from p(beta|sigma2,X,y) : cond. post. dstn of beta
sample.beta &lt;- function(sigma2,X,y){
  V.beta &lt;- solve(t(X)%*%X)             
  beta.hat &lt;- V.beta%*%t(X)%*%y
  return(mvrnorm(1,beta.hat,sigma2*V.beta))
}

#Function for sampling from p(sigma2|beta,X,y) : marginal post. dstn of sigma_squared
sample.sigma2.cond &lt;- function(X,y,N,beta){
  par1=N/2
  par2=0.5*t(y-X%*%beta)%*%(y-X%*%beta)
  return(1/rgamma(1,shape=par1,rate=par2))
}</code></pre>
<p><br />
</p>
</div>
<div id="sampling-1" class="section level3">
<h3>Sampling</h3>
<pre class="r"><code>#Initialize variables for the posterior samples
nsim=1000
beta.post.samples = matrix(0,nsim,k)
sigma2.cond.post.samples = rep(0,nsim)

for(i in 1:nsim){
  beta.post.samples[i,]=sample.beta(sigma2.post.samples[i],X.int,logtime)
  sigma2.cond.post.samples[i]=sample.sigma2.cond(X.int,logtime,N,beta.post.samples[i,])
}</code></pre>
<p><br />
</p>
</div>
<div id="visualization-7" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>dev.new()
par(mfrow=c(2,1))
hist(sigma2.cond.post.samples)

plot(1:nsim,sigma2.cond.post.samples,type=&quot;l&quot;)</code></pre>
<div id="these-are-the-results-of-the-samples-from-the-cond.-post.-dstn-of-sigma-squared" class="section level4">
<h4>These are the results of the samples from the cond. post. dstn of sigma squared</h4>
<p><br />
</p>
</div>
</div>
<div id="bayes-estimate-of-sigma_squared" class="section level3">
<h3>Bayes estimate of sigma_squared</h3>
<pre class="r"><code>mean(sigma2.post.samples)</code></pre>
<pre><code>## [1] 0.4342814</code></pre>
<p><br />
</p>
</div>
<div id="bayes-estimate-of-betas" class="section level3">
<h3>Bayes estimate of betas</h3>
<pre class="r"><code>colMeans(beta.post.samples)</code></pre>
<pre><code>## [1]  0.4372066  0.2640540 -0.6575511  0.5059749</code></pre>
<p><br />
</p>
</div>
<div id="comparison-to-the-freqs-estimates-1" class="section level3">
<h3>Comparison to the freq’s estimates</h3>
<pre class="r"><code>lm.fit$coefficients  # The betas</code></pre>
<pre><code>## (Intercept)     nesting        size      status 
##   0.4308716   0.2650140  -0.6521982   0.5041655</code></pre>
<pre class="r"><code>(summary(lm.fit)$sigma)^2  # Sigma_squared</code></pre>
<pre><code>## [1] 0.4256367</code></pre>
<div id="again-we-can-see-that-both-bayes-and-freqs-results-are-similar.-again-we-can-say-that-out-bayes-model-is-doing-well-i-guess." class="section level4">
<h4>Again, we can see that both bayes and freq’s results are similar. Again, we can say that out bayes model is doing well I guess.</h4>
<p><br />
<br />
</p>
</div>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<div id="samples-from-the-blinreg-are-almost-the-same-as-samples-using-the-marginal-post.-dstn-or-the-samples-using-the-cond.-post.-dstn-which-are-all-in-turn-very-similar-to-the-freqs-result-due-to-using-noninformative-prior." class="section level4">
<h4>Samples from the blinreg() are almost the same as samples using the marginal post. dstn or the samples using the cond. post. dstn, which are all in turn very similar to the freq’s result (due to using noninformative prior).</h4>
<p><br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="example-2" class="section level2">
<h2>Example 2</h2>
<p><img src="images/IMG_0380.jpeg" /></p>
<p><br />
</p>
<div id="loading-data-1" class="section level3">
<h3>Loading data</h3>
<pre class="r"><code>baby=read.table(&quot;http://www.stat.berkeley.edu/~statlabs/data/babies.data&quot;,head=TRUE)
attach(baby)</code></pre>
<p><br />
</p>
</div>
<div id="performing-bayesian-linear-regression-using-mcmcregress" class="section level3">
<h3>Performing bayesian linear regression using MCMCregress()</h3>
<pre class="r"><code>library(MCMCpack)  # for MCMCregress
library(coda)  # for mcmc

options(scipen = 1)  # to remove scientific notation

reg&lt;-MCMCregress(bwt~age+weight,b0=0,B0=0,burnin=1000,mcmc=10000)
summary(reg)</code></pre>
<pre><code>## 
## Iterations = 1001:11000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##                   Mean        SD   Naive SE Time-series SE
## (Intercept) 116.683971  2.283267 0.02283267     0.02279083
## age           0.074110  0.080186 0.00080186     0.00080328
## weight        0.005596  0.003552 0.00003552     0.00003552
## sigma2      332.647666 13.449063 0.13449063     0.13449063
## 
## 2. Quantiles for each variable:
## 
##                   2.5%        25%        50%        75%     97.5%
## (Intercept) 112.280607 115.129890 116.674374 118.241310 121.16086
## age          -0.082259   0.019546   0.074589   0.128984   0.22803
## weight       -0.001408   0.003216   0.005631   0.007999   0.01249
## sigma2      307.227120 323.355276 332.277383 341.446596 360.23436</code></pre>
<div id="we-can-use-either-the-median50-or-the-mean-as-the-bayes-estimate" class="section level4">
<h4>We can use either the median(50%) or the mean as the bayes estimate</h4>
<p><br />
</p>
</div>
</div>
<div id="visualization-8" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>dev.new()
plot(reg)</code></pre>
<div id="we-can-see-that-the-density-of-the-variables-look-like-normal-dstn-so-we-can-use-either-mean-or-median." class="section level4">
<h4>We can see that the density of the variables look like normal dstn so we can use either mean or median.</h4>
<p><br />
</p>
</div>
</div>
<div id="comparison-to-freq." class="section level3">
<h3>Comparison to freq.</h3>
<pre class="r"><code>freq=lm(bwt~age+weight,data=baby)
summary(freq)</code></pre>
<pre><code>## 
## Call:
## lm(formula = bwt ~ age + weight, data = baby)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -65.06 -11.11   0.44  11.25  56.90 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 116.693894   2.294315  50.862   &lt;2e-16 ***
## age           0.074025   0.080475   0.920    0.358    
## weight        0.005564   0.003514   1.584    0.114    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.23 on 1233 degrees of freedom
## Multiple R-squared:  0.002871,   Adjusted R-squared:  0.001254 
## F-statistic: 1.775 on 2 and 1233 DF,  p-value: 0.1699</code></pre>
<pre class="r"><code>(summary(freq)$sigma)^2</code></pre>
<pre><code>## [1] 332.1512</code></pre>
<div id="we-can-see-that-the-results-are-very-similar" class="section level4">
<h4>We can see that the results are very similar</h4>
<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</div>
</div>
</div>
<div id="logistic-regression-for-binary-response" class="section level2">
<h2>Logistic regression for binary response</h2>
<p><br />
</p>
<p><img src="images/ss3.png" /></p>
<p><br />
</p>
<div id="loading-data-2" class="section level3">
<h3>Loading data</h3>
<pre class="r"><code>library(MASS)
data(birthwt)
attach(birthwt)</code></pre>
<pre><code>## The following objects are masked from baby:
## 
##     age, bwt, smoke</code></pre>
<pre class="r"><code>head(birthwt)</code></pre>
<pre><code>##    low age lwt race smoke ptl ht ui ftv  bwt
## 85   0  19 182    2     0   0  0  1   0 2523
## 86   0  33 155    3     0   0  0  0   3 2551
## 87   0  20 105    1     1   0  0  0   1 2557
## 88   0  21 108    1     1   0  0  1   2 2594
## 89   0  18 107    1     1   0  0  1   0 2600
## 91   0  21 124    3     0   0  0  0   0 2622</code></pre>
<div id="low-is-the-binary-response-for-low-birth-weight" class="section level4">
<h4>‘low’ is the binary response for low birth weight</h4>
<p><br />
</p>
</div>
</div>
<div id="post.-dstn-of-beta-with-prior-dstn-for-beta-as-default-improper-uniform-prior" class="section level3">
<h3>Post. dstn of beta (with prior dstn for beta as default improper uniform prior)</h3>
<pre class="r"><code>library(coda)  # for mcmc
library(MCMCpack)  # for MCMClogit

# default improper uniform prior
logit.1 &lt;- MCMClogit(low~age+as.factor(race)+smoke, data=birthwt, beta.start=rep(0.7,5))
# I think you have to make categorical variables into a factor first
# However, smoke is a binary variable, and you don&#39;t have to factor a binary variable

# beta.start is the starting values for the beta vector. There are total 5 betas including the intercept: intercept, age, race2, race3, smoke

# Here, b0 and B0 values were not given =&gt; this is an improper uniform prior

summary(logit.1)</code></pre>
<pre><code>## 
## Iterations = 1001:11000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##                     Mean      SD  Naive SE Time-series SE
## (Intercept)      -0.9883 0.89476 0.0089476       0.036991
## age              -0.0382 0.03425 0.0003425       0.001416
## as.factor(race)2  1.0488 0.51366 0.0051366       0.021548
## as.factor(race)3  1.0972 0.42240 0.0042240       0.019524
## smoke             1.1453 0.38656 0.0038656       0.015945
## 
## 2. Quantiles for each variable:
## 
##                      2.5%     25%      50%      75%   97.5%
## (Intercept)      -2.66847 -1.6161 -1.00037 -0.35902 0.82157
## age              -0.10923 -0.0607 -0.03668 -0.01464 0.02724
## as.factor(race)2  0.02845  0.6990  1.04471  1.40430 2.04175
## as.factor(race)3  0.26664  0.7974  1.10880  1.38758 1.91430
## smoke             0.37771  0.8948  1.13697  1.41022 1.91549</code></pre>
</div>
<div id="visualization-9" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>dev.new()
plot(logit.1)</code></pre>
<p><br />
</p>
</div>
<div id="convergence-diagnostic" class="section level3">
<h3>Convergence Diagnostic</h3>
<pre class="r"><code>logit.1&lt;-MCMClogit(low~age+as.factor(race)+smoke, data=birthwt,beta.start=rep(-0.7,5))
logit.2&lt;-MCMClogit(low~age+as.factor(race)+smoke, data=birthwt,beta.start=rep(-0.3,5))
logit.3&lt;-MCMClogit(low~age+as.factor(race)+smoke, data=birthwt,beta.start=rep(0,5))
logit.4&lt;-MCMClogit(low~age+as.factor(race)+smoke, data=birthwt,beta.start=rep(0.3,5))
logit.5&lt;-MCMClogit(low~age+as.factor(race)+smoke, data=birthwt,beta.start=rep(0.7,5))

logit.list&lt;-mcmc.list(list(logit.1,logit.2,logit.3,logit.4,logit.5))
gelman.diag(logit.list)</code></pre>
<pre><code>## Potential scale reduction factors:
## 
##                  Point est. Upper C.I.
## (Intercept)               1          1
## age                       1          1
## as.factor(race)2          1          1
## as.factor(race)3          1          1
## smoke                     1          1
## 
## Multivariate psrf
## 
## 1</code></pre>
<div id="since-the-potential-scale-reduction-factors-for-each-variable-are-all-below-1.1-we-can-say-that-the-mcmc-convergence-is-good-for-each-variable." class="section level4">
<h4>Since the potential scale reduction factors for each variable are all below 1.1, we can say that the MCMC convergence is good for each variable.</h4>
<pre class="r"><code>gelman.plot(logit.list)</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-125-1.png" width="672" /></p>
</div>
<div id="the-plots-indicate-good-convergence-as-well." class="section level4">
<h4>The plots indicate good convergence as well.</h4>
<p><br />
</p>
</div>
</div>
<div id="post.-dstn-of-beta-with-prior-dstn-for-beta-as-mvn01000i" class="section level3">
<h3>Post. dstn of beta (with prior dstn for beta as MVN(0,1000I))</h3>
<div id="large-variance-noninformative-prior" class="section level4">
<h4>(large variance =&gt; noninformative prior)</h4>
<pre class="r"><code>## multivariate normal prior
logit.2 &lt;- MCMClogit(low~age+as.factor(race)+smoke, b0=0, B0=0.001,data=birthwt)
# b0 is the prior mean of the MVN
# B0 is the prior precision(= 1/prior variance) of the MVN
# This means that the prior variance we used here is 1000
# Since the prior variance is very large, this is a noninformative prior

summary(logit.2)</code></pre>
<pre><code>## 
## Iterations = 1001:11000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##                      Mean      SD  Naive SE Time-series SE
## (Intercept)      -1.04275 0.91241 0.0091241       0.038374
## age              -0.03591 0.03414 0.0003414       0.001384
## as.factor(race)2  1.03163 0.49963 0.0049963       0.020275
## as.factor(race)3  1.08843 0.41973 0.0041973       0.017280
## smoke             1.13528 0.38761 0.0038761       0.015654
## 
## 2. Quantiles for each variable:
## 
##                     2.5%      25%      50%      75%   97.5%
## (Intercept)      -2.7988 -1.69138 -1.06366 -0.40923 0.79022
## age              -0.1074 -0.05856 -0.03523 -0.01299 0.03033
## as.factor(race)2  0.0360  0.69870  1.02284  1.37216 2.01679
## as.factor(race)3  0.2908  0.80183  1.08516  1.36941 1.92019
## smoke             0.3795  0.87363  1.13227  1.40031 1.92418</code></pre>
</div>
</div>
<div id="visualization-10" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>plot(logit.2)</code></pre>
<p><img src="Bayesian_files/figure-html/unnamed-chunk-127-1.png" width="672" /><img src="Bayesian_files/figure-html/unnamed-chunk-127-2.png" width="672" /></p>
<p><br />
<br />
<br />
</p>
</div>
</div>
<div id="poisson-regression-for-count-data" class="section level2">
<h2>Poisson regression for count data</h2>
<p><img src="images/ss4.png" /></p>
<p><br />
</p>
<div id="loading-data-3" class="section level3">
<h3>Loading data</h3>
<pre class="r"><code>data(epil)
attach(epil)</code></pre>
<pre><code>## The following object is masked _by_ .GlobalEnv:
## 
##     y</code></pre>
<pre><code>## The following object is masked from birthwt:
## 
##     age</code></pre>
<pre><code>## The following object is masked from baby:
## 
##     age</code></pre>
<pre class="r"><code>head(epil)</code></pre>
<pre><code>##   y     trt base age V4 subject period      lbase       lage
## 1 5 placebo   11  31  0       1      1 -0.7563538 0.11420370
## 2 3 placebo   11  31  0       1      2 -0.7563538 0.11420370
## 3 3 placebo   11  31  0       1      3 -0.7563538 0.11420370
## 4 3 placebo   11  31  1       1      4 -0.7563538 0.11420370
## 5 3 placebo   11  30  0       2      1 -0.7563538 0.08141387
## 6 5 placebo   11  30  0       2      2 -0.7563538 0.08141387</code></pre>
<p><br />
</p>
</div>
<div id="post.-dstn-of-beta" class="section level3">
<h3>Post. dstn of beta</h3>
<pre class="r"><code>## MCMCpack
pois=MCMCpoisson(y~lbase*trt+lage+V4,data = epil)
summary(pois)</code></pre>
<pre><code>## 
## Iterations = 1001:11000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##                       Mean      SD  Naive SE Time-series SE
## (Intercept)         1.8952 0.04213 0.0004213       0.001910
## lbase               0.9481 0.04234 0.0004234       0.001898
## trtprogabide       -0.3494 0.06029 0.0006029       0.002746
## lage                0.8948 0.11668 0.0011668       0.005371
## V4                 -0.1578 0.05384 0.0005384       0.002402
## lbase:trtprogabide  0.5657 0.06225 0.0006225       0.002771
## 
## 2. Quantiles for each variable:
## 
##                       2.5%     25%     50%     75%    97.5%
## (Intercept)         1.8130  1.8680  1.8966  1.9221  1.97764
## lbase               0.8648  0.9192  0.9490  0.9756  1.03506
## trtprogabide       -0.4742 -0.3864 -0.3465 -0.3100 -0.23288
## lage                0.6695  0.8168  0.8912  0.9689  1.14595
## V4                 -0.2682 -0.1920 -0.1567 -0.1221 -0.05663
## lbase:trtprogabide  0.4407  0.5227  0.5664  0.6059  0.68952</code></pre>
</div>
<div id="visualization-11" class="section level3">
<h3>Visualization</h3>
<pre class="r"><code>dev.new()
plot(pois)</code></pre>
<p><br />
</p>
</div>
<div id="comparison-to-the-freq.-result" class="section level3">
<h3>Comparison to the Freq. result</h3>
<pre class="r"><code>summary(glm(y ~ lbase*trt + lage + V4, family = poisson, data = epil), cor = TRUE)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ lbase * trt + lage + V4, family = poisson, 
##     data = epil)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.0915  -1.4126  -0.2739   0.7580  10.7711  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         1.89791    0.04260  44.552  &lt; 2e-16 ***
## lbase               0.94862    0.04360  21.759  &lt; 2e-16 ***
## trtprogabide       -0.34588    0.06100  -5.670 1.42e-08 ***
## lage                0.88760    0.11650   7.619 2.56e-14 ***
## V4                 -0.15977    0.05458  -2.927  0.00342 ** 
## lbase:trtprogabide  0.56154    0.06352   8.841  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 2517.83  on 235  degrees of freedom
## Residual deviance:  869.07  on 230  degrees of freedom
## AIC: 1647
## 
## Number of Fisher Scoring iterations: 5
## 
## Correlation of Coefficients:
##                    (Intercept) lbase trtprogabide lage  V4   
## lbase              -0.56                                     
## trtprogabide       -0.63        0.39                         
## lage               -0.18       -0.01  0.06                   
## V4                 -0.28        0.00  0.00         0.00      
## lbase:trtprogabide  0.33       -0.69 -0.60         0.32  0.00</code></pre>
<div id="we-can-see-that-the-results-are-very-similar." class="section level4">
<h4>We can see that the results are very similar.</h4>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
